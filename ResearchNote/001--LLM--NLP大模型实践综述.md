# 001--LLM--NLP大模型实践综述

### 1、参考资料

1. [Examining Emergent Abilities in Large Language Models](https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models?utm_source=twitter&utm_medium=social&utm_content=Stanford%20HAI_twitter_StanfordHAI_202303011701_sf175393634&utm_campaign=&sf175393634=1)--from Stanford University Human-Centered Artifial Inteligence
   - [考察大型语言模型中的涌现能力](https://openreview.net/pdf?id=yzkSU5zdwD)





## 大型语言模型中的涌现能力

- Paper--[Emergent Abilities of Large Language Models](https://openreview.net/pdf?id=yzkSU5zdwD)

在过去的二十年里，人工智能领域的大部分研究都集中在训练神经网络来执行单一的特定任务（例如，对一张图片是否包含一只猫进行分类，对一篇新闻文章进行总结，将英语翻译成斯瓦希里语），并为该任务提供标记的训练数据。近年来，一种新的范式已经围绕着语言模型发展起来--神经网络可以简单地预测一个句子中的下一个词，并给出该句子中的前几个词。

在使用这一目标对大型无标签文本语料库进行训练后，语言模型可以被 "提示 "执行任意的任务，并被设定为下一个单词预测。例如，将一个英语短语翻译成斯瓦希里语的任务可以被重新规划为下一个词的预测: "The Swahili translation of 'artificial intelligence' is ..."

这种新的范式代表了从特定任务的模型，即为完成单一任务而训练的模型，到可以完成许多任务的通用模型的转变。任务通用模型甚至可以执行其训练数据中没有明确包括的新任务。例如，GPT-3显示，语言模型可以成功地进行两位数的乘法运算，尽管它们没有被明确地训练成这样。然而，这种执行新任务的能力只发生在有一定数量参数并在足够大的数据集上训练的模型上。

一个系统中的量化变化（如扩大语言模型）可以导致新的行为，这一观点被称为涌现，这一概念由诺贝尔奖得主菲利普-安德森1972年的文章《[More is Different](https://www.science.org/doi/epdf/10.1126/science.177.4047.393)》所推广。在物理学、生物学、经济学和计算机科学等许多学科的复杂系统中都观察到了涌现现象。

在最近发表在Transactions on Machine Learning Research上的一篇论文中，我们将大型语言模型中的突发能力定义为以下内容：

> 如果一种能力在较小的模型中不存在，但在较大的模型中存在，那么这种能力就是涌现的。

为了描述涌现能力的存在，我们的论文汇总了自GPT-3发布以来，在过去两年中出现的各种模型和方法的结果。这篇论文研究分析了规模的影响--用不同的计算资源训练不同规模的模型。对于许多任务，模型的行为要么是可预测地随规模增长，要么是不可预测地从随机性能激增到特定规模阈值的随机以上。

下图显示了三个新兴能力的例子。只有对于具有足够规模的模型（在这种情况下，我们用训练FLOPs来衡量规模），进行算术、参加大学水平的考试（多任务NLU）和识别一个词的预定含义的能力都变得非随机。最关键的是，仅仅从较小的模型的性能来推断，性能的突然增加是不可预测的。

<img src="D:\Onedrive\Work-Documents\NLP-LLM\imgs\imgs_001\Figure_1.png" alt="Figure_1" style="zoom:67%;" />

另一类涌现的能力包括增强语言模型能力的提示策略。这些策略是新兴的，因为较小的模型不能成功地使用这些策略--只有足够大的语言模型可以。  

在规模上出现的提示策略的一个例子是 "思维链提示"，即提示模型在给出最终答案之前产生一系列的中间步骤，类似于 "思维链"。下图（A）对思维链提示进行了总结--它极大地提高了大型语言模型的推理能力，使它们能够解决需要抽象推理的多步骤问题，如数学单词问题。如图（B）所示，在小学数学问题的基准上，思维链提示的表现比直接返回最终答案要差，直到临界大小（10^22 FLOPs），在这一点上它的表现要好很多。

![Figure_2](D:\Onedrive\Work-Documents\NLP-LLM\imgs\imgs_001\Figure_2.png)

涌现的能力具有科学意义，并激励着未来对大型语言模型的研究。语言模型的更多扩展会导致进一步的涌现能力吗？为什么缩放能解锁涌现能力？除了缩放之外，是否有其他方法来释放这种能力？除了出现的能力，是否还有可能出现的新风险，比如可能只存在于未来语言模型中的行为？当某些能力出现时，语言模型的新的现实世界应用是否会被解锁？这些都是许多开放的问题，我们鼓励社区更深入地研究涌现的奥秘。